# âš•ï¸ Clinical Trial Semantic Search Engine

## ğŸ“– Overview

Clinical trial protocols contain over **3.5 million unstructured eligibility criteria**, making it nearly impossible for doctors and patients to find relevant trials using simple keyword search.  
A query for `"diabetes"` would miss trials that list `"HbA1c > 6.5%"`, and a search for `"patients without hypertension"` would fail completely.

This project is an **end-to-end AI-driven system** that solves this problem. It uses **Natural Language Processing (NLP)** and a **fine-tuned Biomedical NER model** to read, understand, and structure all 3.5 million criteria.

The final result is an intelligent **Semantic Search Engine** that can understand complex queries (e.g., _â€œFind trials for patients with lung cancer but no history of heart failureâ€_) and return the most clinically relevant results in milliseconds.

This system bridges the gap between raw, unstructured text and a structured, searchable knowledge base, making clinical trial data:

- ğŸ©º **Accessible** for physicians matching patients  
- ğŸ”¬ **Computable** for researchers analyzing trends  
- ğŸ“Š **Scalable** for repositories managing millions of documents  

---

## ğŸ¯ Problem Statement

Clinical trial eligibility criteria are written in complex, free-form medical language. This data is:

- âŒ **Unstructured:** A mix of full sentences, bullet points, and medical jargon  
- âŒ **Ambiguous:** `> 50%` means nothing without knowing it refers to LVEF  
- âŒ **Unsearchable:** Keyword-based search fails to capture semantic meaning  

### âœ… This project solves it by building a 5-phase pipeline that:
- âœ”ï¸ Segments 3.5M criteria into individual sentences  
- âœ”ï¸ Extracts 7 types of key medical entities using a fine-tuned BioBERT model  
- âœ”ï¸ Structures this data by linking entities together (e.g., LAB_TEST â†’ VALUE)  
- âœ”ï¸ Indexes the semantic meaning of every criterion in a high-speed Vector Database  
- âœ”ï¸ Deploys the system as a simple, fast **FastAPI** search endpoint  

---

## ğŸš€ Key Features

- ğŸ§  **Biomedical NER:** `dmis-lab/biobert-base-cased-v1.1` fine-tuned to detect 7 entity types  
- ğŸ›ï¸ **Knowledge Base Creation:** Extracts structured entities from 3.5M criteria into JSONL  
- ğŸ”— **Relation Extraction:** Rule-based spaCy system links LAB_TESTS with VALUES and OPERATORS  
- ğŸ·ï¸ **Entity Normalization:** Maps extracted text (e.g., â€œNSCLCâ€) to UMLS codes  
- âš¡ **High-Speed Semantic Search:** Uses `all-MiniLM-L6-v2` for embedding-based semantic matching  
- ğŸ—„ï¸ **Vector Database:** Built with **ChromaDB** for millisecond-scale cosine similarity search  
- ğŸ”Œ **API-Ready:** FastAPI `/search/` endpoint for external applications  

---

## ğŸ—ï¸ System Architecture & Methodology

### **Phase I: Data Ingestion & Segmentation**

**Goal:** Ingest 3.5M+ raw criteria and segment them into clean sentences.  
**Scripts:** `ingest_data.py`, `segmentation.py`, `01_preprocess_data.py`

**Action:**  
Uses `spaCy`â€™s `nlp.pipe()` for efficient batch processing. The script cleans and segments text, outputting `train.csv` and `test.csv`.

**Equation (Self-Attention):**

```math
Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
````

---

### **Phase II: NER Model Fine-Tuning (Core Intelligence Engine)**

**Goal:** Train BioBERT to recognize 7 custom biomedical entities.
**Scripts:** `create_annotation_sample.py`, `prepare_ner_data.py`, `train_ner_model.py`, `test_ner_model.py`

#### Models & Training Details

* **Model:** `dmis-lab/biobert-base-cased-v1.1`
* **Loss Function:** Cross-Entropy Loss

```math
J(\theta) = - \frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c} \log(p_{i,c})
```

* **Evaluation Metrics:** Precision, Recall, F1

```math
P = \frac{TP}{TP + FP} \\
R = \frac{TP}{TP + FN} \\
F1 = 2 \times \frac{P \times R}{P + R}
```

**Output:** Fine-tuned model saved to `models/clinical-ner-model/`

---

### **Phase III: Structuring & Normalization**

**Goal:** Apply trained model on all 3.5M+ sentences and structure the results.
**Scripts:** `apply_ner_model.py`, `structure_and_normalize.py`

#### Techniques:

* **Rule-Based Relation Extraction:** Uses dependency parsing in spaCy
* **Entity Normalization:** Python dictionary maps entity to UMLS codes
* **Example Rule:**

  * If a LAB_TEST has a negation child â†’ add `is_negated: True`
  * If a LAB_TEST links to VALUE â†’ add `related_value`

**Output:** `train_structured_knowledge.jsonl`

---

### **Phase IV: Vectorization & Indexing**

**Goal:** Build the semantic search index.

**Script:** `vectorize_and_index.py`
**Model:** `all-MiniLM-L6-v2` (384D vector embeddings)
**Database:** `ChromaDB`

#### Search Equation (Cosine Similarity)

```math
Similarity(Q, D) = \frac{Q \cdot D}{\|Q\| \|D\|}
```

**Output:** Stored in `vector_db/` for high-speed kNN search.

---

### **Phase V: API Deployment**

**Goal:** Serve the search engine through an API.
**Script:** `api.py`

#### Endpoint:

```
POST http://127.0.0.1:8000/search/
```

**Example Request:**

```json
{"query_text": "patients with diabetes but no heart failure"}
```

**Example Response:**

```json
{
  "query": "patients with diabetes but no heart failure",
  "top_k_results": [
    {
      "nct_id": "NCT00055314",
      "criterion_text": "Exclusion of patients with uncontrolled diabetes.",
      "similarity_score": 0.8521
    }
  ]
}
```

---

## ğŸ› ï¸ Tech Stack

| Category           | Tools & Libraries                   |
| ------------------ | ----------------------------------- |
| **Core Language**  | Python 3.11                         |
| **Data Science**   | pandas, numpy, matplotlib, seaborn  |
| **NLP**            | spaCy                               |
| **Deep Learning**  | PyTorch                             |
| **Model Pipeline** | Hugging Face transformers, datasets |
| **Embeddings**     | sentence-transformers               |
| **Vector DB**      | chromadb                            |
| **API Server**     | FastAPI, uvicorn                    |
| **Evaluation**     | seqeval                             |
| **Utilities**      | tqdm, json                          |

---

## ğŸ“Š Results & Visualizations

1. **Training Performance:**

   * Model trained for 3 epochs on CPU
   * Decreasing training loss confirmed learning
   * Final Macro F1 â‰ˆ **0.23**

2. **Entity Distribution:**

   * `VALUE` most frequent
   * Followed by `CONDITION` and `LAB_TEST`

3. **Example Predictions:**

```
Text: 'No prior chemotherapy within the last 6 months.'
Entities:
 - 'no' â†’ OPERATOR
 - 'chemotherapy' â†’ DRUG
 - 'within the last 6 months' â†’ VALUE
```

---

## ğŸ“‚ Repository Structure

```
clinical_trials_ner/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â”œâ”€â”€ test.csv
â”‚   â”‚   â”œâ”€â”€ sample.txt
â”‚   â”‚   â”œâ”€â”€ all.jsonl
â”‚   â”‚   â”œâ”€â”€ train_with_entities.jsonl
â”‚   â”‚   â””â”€â”€ train_structured_knowledge.jsonl
â”‚   â””â”€â”€ ner_dataset/
â”‚
â”œâ”€â”€ plots/
â”‚   â”œâ”€â”€ training_loss_curve.png
â”‚   â”œâ”€â”€ final_evaluation_metrics.png
â”‚   â””â”€â”€ entity_distribution.png
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest_data.py
â”‚   â”œâ”€â”€ segmentation.py
â”‚   â”œâ”€â”€ preprocess_data.py
â”‚   â”œâ”€â”€ create_annotation_sample.py
â”‚   â”œâ”€â”€ prepare_ner_data.py
â”‚   â”œâ”€â”€ train_ner_model.py
â”‚   â”œâ”€â”€ test_ner_model.py
â”‚   â”œâ”€â”€ apply_ner_model.py
â”‚   â”œâ”€â”€ structure_and_normalize.py
â”‚   â”œâ”€â”€ visualize_results.py
â”‚   â”œâ”€â”€ vectorize_and_index.py
â”‚   â””â”€â”€ api.py
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ clinical-ner-model/
â”‚
â””â”€â”€ vector_db/
```

---

## ğŸ“¥ Installation

```bash
# 1. Clone the repository
git https://github.com/ss1910singh/Clinical-Trial-Semantic-Search-Engine.git
cd Clinical-Trial-Semantic-Search-Engine

# 2. Create and activate a virtual environment
python -m venv venv
source venv/bin/activate  # (Windows: venv\Scripts\activate)

# 3. Install dependencies
pip install -r requirements.txt

# 4. Download the spaCy model
python -m spacy download en_core_web_sm
```

---

## â–¶ï¸ How to Run the Pipeline

```bash
# Phase I: Preprocessing (~10-15 min)
python scripts/preprocess_data.py

# Phase II: Create and train the NER model
python scripts/create_annotation_sample.py
# (Annotate 'sample.txt' manually, save as 'all.jsonl')
python scripts/prepare_ner_data.py
python scripts/train_ner_model.py

# Optional: Test & visualize
python scripts/test_ner_model.py
python scripts/visualize_results.py

# Phase III: Apply NER and structure data
python scripts/apply_ner_model.py
python scripts/structure_and_normalize.py

# Phase IV: Build vector database
python scripts/vectorize_and_index.py

# Phase V: Launch FastAPI server
python scripts/api.py
```

Access the docs at [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

---

## ğŸ§© Future Work

* Expand annotated dataset for higher NER accuracy
* Add relation classification using transformers
* Integrate ontology-based query expansion
* Support multilingual biomedical datasets

---

## ğŸ‘¨â€âš•ï¸ Authors & Credits

Developed by

* **Satish Singh**
* **Pranavi Shukla**
* **Aakash Shedge**
* **Sankalp Sathe**

**Mentor:** *Dr. Ekta Sarda*
